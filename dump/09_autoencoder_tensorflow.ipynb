{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement autoencoder based on lstm, cnn and dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with time series data, using dense layers (fully connected layers) with an input size equal to the sequence length is generally not ideal for several reasons:\n",
    "\n",
    "1. Loss of Temporal Dependencies\n",
    "Time series data has an inherent sequential nature where the order and timing of data points matter. Dense layers treat each input feature independently, ignoring the temporal dependencies between consecutive time points. This can lead to poor performance in capturing the patterns and relationships that are crucial in time series data.\n",
    "\n",
    "2. Fixed Input Size\n",
    "Dense layers require a fixed input size, meaning the model architecture is rigid and cannot easily adapt to sequences of varying lengths. In many real-world applications, time series data can have varying lengths, necessitating a more flexible approach.\n",
    "\n",
    "3. Parameter Inefficiency\n",
    "Dense layers with a large input size result in a vast number of parameters, making the model computationally expensive and prone to overfitting, especially when the sequence length is long. This inefficiency can be a significant drawback when working with high-dimensional time series data.\n",
    "\n",
    "4. Ineffective for Long-Term Dependencies\n",
    "Dense layers are not well-suited for capturing long-term dependencies within the data. Techniques like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs) are specifically designed to handle sequences and can remember information over long periods, making them more effective for time series analysis.\n",
    "\n",
    "5. Lack of Temporal Feature Extraction\n",
    "Dense layers do not have mechanisms to extract meaningful temporal features like trends and seasonal patterns inherent in time series data. Convolutional Neural Networks (CNNs) with temporal convolutions or RNNs/LSTMs/GRUs can better capture these features by applying convolutional or recurrent operations over the sequence.\n",
    "\n",
    "Alternative Approaches\n",
    "For these reasons, the following approaches are generally preferred for time series data:\n",
    "\n",
    "Recurrent Neural Networks (RNNs): Capture temporal dependencies by maintaining a hidden state that gets updated at each time step.\n",
    "Long Short-Term Memory (LSTM) Networks: Address the vanishing gradient problem of RNNs and can capture long-term dependencies.\n",
    "Gated Recurrent Units (GRUs): A simplified version of LSTMs that can also handle long-term dependencies.\n",
    "Convolutional Neural Networks (CNNs): Can be applied to time series data to capture local temporal patterns through temporal convolutions.\n",
    "Temporal Convolutional Networks (TCNs): Use dilated convolutions to capture long-range dependencies while maintaining computational efficiency.\n",
    "These approaches are designed to leverage the sequential nature of time series data, leading to better performance in tasks such as forecasting, classification, and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 16:32:40.074853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-22 16:32:40.106728: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-22 16:32:40.115605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-22 16:32:40.137424: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 16:32:41.317023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721658762.507714   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658762.572471   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658762.573009   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])  # Set the memory limit as needed\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_0</th>\n",
       "      <th>data_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51840.000000</td>\n",
       "      <td>51840.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.428187</td>\n",
       "      <td>27.427566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.276855</td>\n",
       "      <td>4.281787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.042714</td>\n",
       "      <td>16.342305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.792250</td>\n",
       "      <td>23.832418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29.712173</td>\n",
       "      <td>29.709107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.188862</td>\n",
       "      <td>30.189345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41.066048</td>\n",
       "      <td>41.122645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             data_0        data_1\n",
       "count  51840.000000  51840.000000\n",
       "mean      27.428187     27.427566\n",
       "std        4.276855      4.281787\n",
       "min       16.042714     16.342305\n",
       "25%       23.792250     23.832418\n",
       "50%       29.712173     29.709107\n",
       "75%       30.188862     30.189345\n",
       "max       41.066048     41.122645"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"times_series_data_no_labels.csv\" ,\n",
    "    index_col='datetime',\n",
    "    parse_dates=['datetime']\n",
    "    )\n",
    "\n",
    "data = data.asfreq('5min')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_0</th>\n",
       "      <th>data_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:00</th>\n",
       "      <td>21.719925</td>\n",
       "      <td>19.925141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:05:00</th>\n",
       "      <td>21.357656</td>\n",
       "      <td>19.671888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:10:00</th>\n",
       "      <td>20.178934</td>\n",
       "      <td>19.543689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:15:00</th>\n",
       "      <td>19.197688</td>\n",
       "      <td>18.872886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:20:00</th>\n",
       "      <td>20.098658</td>\n",
       "      <td>19.599005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 23:35:00</th>\n",
       "      <td>19.636588</td>\n",
       "      <td>20.640584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 23:40:00</th>\n",
       "      <td>20.692796</td>\n",
       "      <td>19.895390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 23:45:00</th>\n",
       "      <td>20.081966</td>\n",
       "      <td>20.584634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 23:50:00</th>\n",
       "      <td>19.956621</td>\n",
       "      <td>20.553717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29 23:55:00</th>\n",
       "      <td>19.974779</td>\n",
       "      <td>20.682379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51840 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        data_0     data_1\n",
       "datetime                                 \n",
       "2023-01-01 00:00:00  21.719925  19.925141\n",
       "2023-01-01 00:05:00  21.357656  19.671888\n",
       "2023-01-01 00:10:00  20.178934  19.543689\n",
       "2023-01-01 00:15:00  19.197688  18.872886\n",
       "2023-01-01 00:20:00  20.098658  19.599005\n",
       "...                        ...        ...\n",
       "2023-06-29 23:35:00  19.636588  20.640584\n",
       "2023-06-29 23:40:00  20.692796  19.895390\n",
       "2023-06-29 23:45:00  20.081966  20.584634\n",
       "2023-06-29 23:50:00  19.956621  20.553717\n",
       "2023-06-29 23:55:00  19.974779  20.682379\n",
       "\n",
       "[51840 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Anomaly Detection with LSTM Autoencoders.\n",
    "\n",
    "In this method we will depend on the detection using the forecasting by Deep Learning algorithms. In the forecasting methods we depend on predict the next point with the addition of some noise and make comparison of this point and the true point at this timestamp by finding the difference between the two points then add threshold finally find the anomalies by compare the difference of the two points with this threshold (we used the Mean absolute error MAE).\n",
    "\n",
    "Autoencoders are type of self-supervised learning model which are a neural network that learn from the input data. We use autoencoder because the Principal Component Analysis (PCA), which we used in the previous method we depend on the linear algebra to do the models, but by using autoencoders we depended on the non-linear transformation like by use the activation functions; those non-linearity gives us the ability to go deep in the number of the neural network layers.\n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of artificial recurrent neural network (RNN). which are designed to handle sequential data, with the previous step's output being fed as the current step's input.\n",
    "\n",
    "We apply some dimensionality reduction on our dataset by use encoder to make the dimension small then use the decoder to get it back and that minimize the reconstruction loss. In fact, that will make us lose some information but it gives us the ability to know the main pattern of the information and thought that we could define any information out hits pattern under sone threshold will be outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44064, 2) (7776, 2)\n",
      "(44034, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "\n",
    "# sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "# rcParams['figure.figsize'] = 22, 10\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Load and prepare data\n",
    "# data = pd.read_csv('/kaggle/input/milan-dataset/final_data.csv', parse_dates=['time'], index_col='time')\n",
    "# data = data.groupby(\"grid_square\").get_group(5056)\n",
    "train_size = int(len(data) * 0.85)\n",
    "test_size = len(data) - train_size\n",
    "train, test = data.iloc[0:train_size], data.iloc[train_size:len(data)]\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[['data_0']])\n",
    "\n",
    "# Transform and explicitly cast to float64\n",
    "train_transformed = scaler.transform(train[['data_0']]).astype('float64')\n",
    "test_transformed = scaler.transform(test[['data_0']]).astype('float64')\n",
    "\n",
    "# Assign the transformed values back to the DataFrame\n",
    "train = pd.DataFrame(train_transformed, columns=['data_0'])\n",
    "test = pd.DataFrame(test_transformed, columns=['data_0'])\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 30\n",
    "\n",
    "# reshape to [samples, time_steps, n_features]\n",
    "\n",
    "X_train, y_train = create_dataset(train[['data_0']], train.data_0, TIME_STEPS)\n",
    "X_test, y_test = create_dataset(test[['data_0']], test.data_0, TIME_STEPS)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# print(X_train.size(), X_test.size(), y_train.size(), y_test.size())\n",
    "# # Create DataLoader\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44034,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1721658766.791834   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658766.792473   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658766.793002   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658766.897298   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658766.897599   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1721658766.897796   98288 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-22 16:32:46.897967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │            \u001b[38;5;34m65\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,985</span> (195.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m49,985\u001b[0m (195.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">49,985</span> (195.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m49,985\u001b[0m (195.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# Create the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add an Input layer\n",
    "model.add(keras.layers.Input(shape=input_shape))\n",
    "\n",
    "# Add the rest of the layers\n",
    "model.add(keras.layers.LSTM(units=64))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\n",
    "model.add(keras.layers.LSTM(units=64, return_sequences=True))\n",
    "model.add(keras.layers.Dropout(rate=0.2))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 16:32:49.790880: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.3262 - val_loss: 0.2794\n",
      "Epoch 2/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2706 - val_loss: 0.2864\n",
      "Epoch 3/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 0.2697 - val_loss: 0.2730\n",
      "Epoch 4/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 0.2717 - val_loss: 0.2827\n",
      "Epoch 5/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2752 - val_loss: 0.2810\n",
      "Epoch 6/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 0.2685 - val_loss: 0.2978\n",
      "Epoch 7/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2616 - val_loss: 0.2856\n",
      "Epoch 8/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2662 - val_loss: 0.2998\n",
      "Epoch 9/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 0.2590 - val_loss: 0.2786\n",
      "Epoch 10/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 12ms/step - loss: 0.2596 - val_loss: 0.2969\n",
      "Epoch 11/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2633 - val_loss: 0.2869\n",
      "Epoch 12/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 0.2585 - val_loss: 0.2972\n",
      "Epoch 13/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 0.2580 - val_loss: 0.2876\n",
      "Epoch 14/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.2619 - val_loss: 0.3014\n",
      "Epoch 15/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - loss: 0.2705 - val_loss: 0.2904\n",
      "Epoch 16/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 0.2668 - val_loss: 0.2929\n",
      "Epoch 17/20\n",
      "\u001b[1m1239/1239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - loss: 0.2580 - val_loss: 0.2948\n",
      "Epoch 18/20\n",
      "\u001b[1m  25/1239\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 14ms/step - loss: 0.2583"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = model.predict(X_test)\n",
    "\n",
    "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "len(test_mae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_mae_loss, bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = model.predict(X_train, verbose=0)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
    "\n",
    "plt.hist(train_mae_loss, bins=50)\n",
    "plt.xlabel('Train MAE loss')\n",
    "plt.ylabel('Number of Samples');\n",
    "\n",
    "threshold = np.max(train_mae_loss)\n",
    "print(f'Reconstruction error threshold: {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)\n",
    "test_score_df['loss'] = test_mae_loss\n",
    "test_score_df['threshold'] = THRESHOLD\n",
    "test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold\n",
    "test_score_df['data_0'] = test[TIME_STEPS:].data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_score_df.index, test_score_df.loss, label='loss')\n",
    "plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')\n",
    "plt.xticks(rotation=25)\n",
    "plt.title('test_score_loss vs. threshold')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = test_score_df[test_score_df.anomaly == True]\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_df['anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(test[TIME_STEPS:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "  test[TIME_STEPS:].index, \n",
    "  scaler.inverse_transform(test[TIME_STEPS:]), \n",
    "  label='data_0'\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=anomalies.index.to_numpy(),\n",
    "    y=scaler.inverse_transform(anomalies[[\"data_0\"]]).reshape(-1),\n",
    "    color=sns.color_palette()[3],\n",
    "    s=52,\n",
    "    label='anomaly'\n",
    ")\n",
    "plt.xticks(rotation=25)\n",
    "plt.title('Anomalies')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months = data.index.to_period('M').unique()\n",
    "# import plotly.subplots as sp\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# plots = []\n",
    "# for month in months:\n",
    "#     monthly_data = data[data.index.to_period('M') == month]\n",
    "\n",
    "#     fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True, \n",
    "#                            subplot_titles=('data_0', 'data_1', 'diff'),\n",
    "#                            vertical_spacing=0.03, horizontal_spacing=0.02)\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x=monthly_data.index, y=monthly_data['data_0'], name='data_0', mode='lines'), row=1, col=1)\n",
    "#     # fig.add_trace(go.Scatter(x=monthly_data.index, y=monthly_data['data_1'], name='data_1', mode='lines'), row=2, col=1)\n",
    "#     # fig.add_trace(go.Scatter(x=monthly_data.index, y=monthly_data['diff'], name='diff', mode='lines'), row=3, col=1)\n",
    "\n",
    "#     # Find the indices where the difference exceeds the threshold\n",
    "#     anomaly_indices = monthly_data[monthly_data['anomalies']].index\n",
    "\n",
    "#     # Add markers for anomalies\n",
    "#     fig.add_trace(go.Scatter(x=anomaly_indices, y=monthly_data.loc[anomaly_indices, 'data_0'], \n",
    "#                              mode='markers', name='Anomaly data_0', \n",
    "#                              marker=dict(color='red', size=10)), row=1, col=1)\n",
    "    \n",
    "#     # fig.add_trace(go.Scatter(x=anomaly_indices, y=monthly_data.loc[anomaly_indices, 'data_1'], \n",
    "#     #                          mode='markers', name='Anomaly data_1', \n",
    "#     #                          marker=dict(color='blue', size=10)), row=2, col=1)\n",
    "    \n",
    "#     fig.update_layout(title_text=f\"Data for {month}\")\n",
    "#     plots.append(fig)\n",
    "\n",
    "# # Display the plots\n",
    "# for plot in plots:\n",
    "#     plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
