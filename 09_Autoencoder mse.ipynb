{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we apply dimensionality reduction to find outliers?\n",
    "Don’t we lose some information, including the outliers, if we reduce the dimensionality? The answer is that once the main patterns are identified, the outliers are revealed. Many distance-based techniques (e.g. KNNs) suffer the curse of dimensionality when they compute distances of every data point in the full feature space. High dimensionality has to be reduced. \n",
    "\n",
    "Interestingly, during the process of dimensionality reduction outliers are identified. We can say outlier detection is a by-product of dimension reduction.\n",
    "\n",
    "Autoencoders are an unsupervised approach to find anomalies.\n",
    "\n",
    "Why autoencoders?\n",
    "There are many useful tools, such as Principal Component Analysis (PCA), for detecting outliers. Why do we need autoencoders? The reason is that PCA uses linear algebra to transform. In contrast, autoencoder techniques can perform non-linear transformations with their non-linear activation function and multiple layers. It’s more efficient to train several layers with an autoencoder, rather than training one huge transformation with PCA. The autoencoder techniques thus show their merits when the data problems are complex and non-linear in nature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import json\n",
    "\n",
    "TIME_STEPS = 30\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "LATENT_DIM = 4\n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "HYPER_PARAMETER_TUNE = False\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "data_org = pd.read_csv(\"times_series_data_no_labels.csv\", index_col='datetime', parse_dates=['datetime'])\n",
    "\n",
    "def preprocess_data(column_name):\n",
    "    # Removing rows with values greater than 32 and less than 19\n",
    "    data = data_org[(data_org[column_name] <= 32) & (data_org[column_name] >= 19)]\n",
    "\n",
    "    # Removing rows for the time between 5:45 and 21:00 with values less than 26\n",
    "    data['hour'] = data.index.hour\n",
    "    data['minute'] = data.index.minute\n",
    "\n",
    "    condition_time = ~((data['hour'] > 5) & ((data['hour'] < 21) | ((data['hour'] == 21) & (data['minute'] == 0))) & (data[column_name] < 26))\n",
    "    data = data[condition_time]\n",
    "\n",
    "    # Dropping the additional columns used for filtering\n",
    "    data.drop(columns=['hour', 'minute'], inplace=True)\n",
    "\n",
    "    # Removing rows for the time between 00:10 and 03:05 with values greater than 22.5\n",
    "    data['hour'] = data.index.hour\n",
    "    data['minute'] = data.index.minute\n",
    "\n",
    "    condition_night = ~((data['hour'] == 0) & (data['minute'] >= 10) |\n",
    "                        (data['hour'] > 0) & (data['hour'] < 3) |\n",
    "                        (data['hour'] == 3) & (data['minute'] <= 5) &\n",
    "                        (data[column_name] > 22.5))\n",
    "    data = data[condition_night]\n",
    "\n",
    "    # Dropping the additional columns used for filtering\n",
    "    data.drop(columns=['hour', 'minute'], inplace=True)\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_size = int(len(data) * 0.85)\n",
    "    train, test = data.iloc[0:train_size], data.iloc[train_size:len(data)]\n",
    "\n",
    "    return train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_0, test_0 = preprocess_data('data_0')\n",
    "train_1, test_1 = preprocess_data('data_1')\n",
    "\n",
    "def create_dataset(X, time_steps=1):\n",
    "    Xs = []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "    return np.array(Xs)\n",
    "\n",
    "def normalize_data(data, min_val, max_val):\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DenseEncoder(nn.Module):\n",
    "    def __init__(self, input_shape: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_features=input_shape, out_features=4 * latent_dim)\n",
    "        self.l2 = nn.Linear(in_features=4 * latent_dim, out_features=2 * latent_dim)\n",
    "        self.l3 = nn.Linear(in_features=2 * latent_dim, out_features=latent_dim)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.l1(inputs)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l3(x)\n",
    "        latent = torch.relu(x)\n",
    "        return latent\n",
    "\n",
    "\n",
    "class DenseDecoder(nn.Module):\n",
    "    def __init__(self, output_shape: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.l4 = nn.Linear(in_features=latent_dim, out_features=2 * latent_dim)\n",
    "        self.l5 = nn.Linear(in_features=2 * latent_dim, out_features=4 * latent_dim)\n",
    "        self.output = nn.Linear(in_features=4 * latent_dim, out_features=output_shape)\n",
    "\n",
    "    def forward(self, latent: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.l4(latent)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l5(x)\n",
    "        x = torch.relu(x)\n",
    "        output = self.output(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class DenseAutoencoderModel(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super(DenseAutoencoderModel, self).__init__()\n",
    "        self.encoder = DenseEncoder(input_shape, latent_dim)\n",
    "        self.decoder = DenseDecoder(input_shape, latent_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.squeeze(2)\n",
    "        latent = self.encoder(inputs)\n",
    "        output = self.decoder(latent)\n",
    "        output = output.unsqueeze(2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for df in train_loader:\n",
    "            df = df[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(df)\n",
    "            loss = criterion(output, df)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss}')\n",
    "\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, train_loader):\n",
    "    latent_dim = trial.suggest_int('latent_dim', 2, 12)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = DenseAutoencoderModel(input_shape=TIME_STEPS, latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    final_loss = train_model(model, train_loader, criterion, optimizer, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "def run_optuna(train_loader):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_loader), n_trials=50)\n",
    "    best_params = study.best_params\n",
    "    print(f'Best hyperparameters: {best_params}')\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_train(train, column_name):\n",
    "    train_col = pd.DataFrame(train, columns=[column_name])\n",
    "\n",
    "    TIME_STEPS = 30\n",
    "    BATCH_SIZE = 512\n",
    "    EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 20\n",
    "\n",
    "    X_train = create_dataset(train_col, TIME_STEPS)\n",
    "\n",
    "    min_val = X_train.min()\n",
    "    max_val = X_train.max()  # Use train max for consistency\n",
    "\n",
    "    train_data = normalize_data(X_train, min_val, max_val)\n",
    "\n",
    "    train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    if HYPER_PARAMETER_TUNE:\n",
    "        best_params = run_optuna(train_loader)\n",
    "\n",
    "        with open(f'saves/best_hyperparameters_{column_name}.json', 'w') as f:\n",
    "            json.dump(best_params, f)\n",
    "\n",
    "        model = DenseAutoencoderModel(input_shape=TIME_STEPS, latent_dim=best_params['latent_dim']).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "        \n",
    "    else:\n",
    "        with open(f'saves/best_hyperparameters_{column_name}.json', 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "        model = DenseAutoencoderModel(input_shape=TIME_STEPS, latent_dim=best_params['latent_dim']).to(device)\n",
    "        # model.load_state_dict(torch.load('saves/best_autoencoder_model_data_0.pth'))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "    final_loss = train_model(model, train_loader, criterion, optimizer, EPOCHS, EARLY_STOPPING_PATIENCE)\n",
    "    print(f'Final model training loss for {column_name}: {final_loss}')\n",
    "\n",
    "    if HYPER_PARAMETER_TUNE:\n",
    "        torch.save(model.state_dict(), f'saves/best_autoencoder_model_{column_name}.pth')\n",
    "        print(f'Best model for {column_name} saved!')\n",
    "\n",
    "    return model, min_val, max_val, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.0020245392638332162\n",
      "Epoch 14, Loss: 0.0019641064680303985\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "os.makedirs('saves', exist_ok=True)\n",
    "\n",
    "# # Train models for data_0 and data_1\n",
    "model_0, min_val_0, max_val_0, best_params_0 = preprocess_and_train(train_0, 'data_0')\n",
    "model_1, min_val_1, max_val_1, best_params_1 = preprocess_and_train(train_1, 'data_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomalies(column_name, model, min_val, max_val, threshold):\n",
    "    # Create a windowed dataset for the specified column\n",
    "    data_window = create_dataset(data_org[[column_name]], TIME_STEPS)\n",
    "    \n",
    "    # Scale the data\n",
    "    data_window_scale = (data_window - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    data_window_scale = torch.tensor(data_window_scale, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    data_loader = torch.utils.data.DataLoader(data_window_scale, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Calculate reconstruction losses\n",
    "    reconstruction_loss = []\n",
    "    with torch.no_grad():\n",
    "        for df in data_loader:\n",
    "            df = df.to(device)\n",
    "            output = model(df)\n",
    "            loss = criterion(output, df)\n",
    "            reconstruction_loss.append(loss.item())\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    array_of_values = np.array(reconstruction_loss)\n",
    "    \n",
    "    # Identify anomalies\n",
    "    is_anomaly = array_of_values > threshold\n",
    "    \n",
    "    # Prepare column name for anomaly flag\n",
    "    anomaly_column = f\"is_anomaly_{column_name.split('_')[1]}\"\n",
    "    data_org[anomaly_column] = False\n",
    "    \n",
    "    # Calculate the starting index for updating the original DataFrame\n",
    "    n = len(is_anomaly)\n",
    "    start_idx = -(n + 5)\n",
    "    if start_idx < 0:\n",
    "        start_idx = max(len(data_org) + start_idx, 0)\n",
    "    \n",
    "    # Get the rows to update\n",
    "    rows_to_update = data_org.index[start_idx:start_idx + n]\n",
    "    \n",
    "    # Update the DataFrame with anomaly information\n",
    "    data_org.loc[rows_to_update, anomaly_column] = is_anomaly\n",
    "    \n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_0 = 0.005  # Set threshold for data_0\n",
    "threshold_1 = 0.005  # Set threshold for data_1\n",
    "\n",
    "reconstruction_loss_0 = calculate_anomalies('data_0', model_0, min_val_0, max_val_0, threshold_0)\n",
    "reconstruction_loss_1 = calculate_anomalies('data_1', model_1, min_val_1, max_val_1, threshold_1)\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(reconstruction_loss_0, bins=100)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Reconstruction Losses for data_0')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(reconstruction_loss_1, bins=100)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Reconstruction Losses for data_1')\n",
    "plt.show()\n",
    "\n",
    "# Plot anomalies\n",
    "from plot_anomaly import univariate_anomaly_plot\n",
    "univariate_anomaly_plot(data=data_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we can distinguish and label pretty perfectly between typical datums and anomalies.\n",
    "\n",
    "Pros\n",
    "\n",
    "Autoencoders can handle high-dimensional data with ease. \n",
    "Pertaining to its nonlinearity behavior, it can find complex patterns within high-dimensional datasets.\n",
    "Cons\n",
    "\n",
    "Since it’s a deep learning-based strategy, it will particularly struggle if the data is less.\n",
    "Computation costs will skyrocket if the depth of the network increases and while dealing with big data.\n",
    "So far we’ve seen how to detect and identify anomalies. But the real question arises after finding them. Now what? What do we do about it?\n",
    "\n",
    "Let’s discuss some of the pointers you could apply in your scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
